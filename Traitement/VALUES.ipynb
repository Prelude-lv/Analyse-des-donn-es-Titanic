{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2184390",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bien sûr, voici le code avec les étapes de gestion des valeurs manquantes, des valeurs aberrantes, de suppression des doublons, de gestion des valeurs anormales et de gestion des valeurs zéro :\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Acquisition des Données\n",
    "data = pd.read_csv('votre_jeu_de_donnees.csv')\n",
    "\n",
    "# 2. Exploration des Données (EDA)\n",
    "# ... (Effectuez votre EDA ici)\n",
    "\n",
    "# 3. Nettoyage des Données\n",
    "# Gestion des valeurs manquantes\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data[['numerical_feature_1', 'numerical_feature_2']] = imputer.fit_transform(data[['numerical_feature_1', 'numerical_feature_2']])\n",
    "\n",
    "# 4. Gestion des Valeurs Aberrantes\n",
    "# Remplacez 'votre_colonne' par le nom de la colonne concernée\n",
    "lower_bound = data['votre_colonne'].quantile(0.05)\n",
    "upper_bound = data['votre_colonne'].quantile(0.95)\n",
    "data['votre_colonne'] = data['votre_colonne'].clip(lower=lower_bound, upper=upper_bound)\n",
    "\n",
    "# 5. Suppression des Duplicatas\n",
    "data = data.drop_duplicates()\n",
    "\n",
    "# 6. Gestion des Valeurs Anormales\n",
    "# Remplacez 'votre_colonne' par le nom de la colonne concernée\n",
    "threshold = 3  # Choisissez un seuil approprié pour votre cas\n",
    "data = data[(data['votre_colonne'] < threshold) & (data['votre_colonne'] > -threshold)]\n",
    "\n",
    "# 7. Gestion des Valeurs Zéro\n",
    "# Remplacez 'votre_colonne' par le nom de la colonne concernée\n",
    "data['votre_colonne'] = data['votre_colonne'].replace(0, data['votre_colonne'].mean())\n",
    "\n",
    "# 8. Normalisation et Standardisation\n",
    "numeric_features = ['numerical_feature_1', 'numerical_feature_2']\n",
    "scaler = StandardScaler()\n",
    "data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "\n",
    "# 9. Encodage des Variables Catégorielles\n",
    "categorical_features = ['categorical_feature_1', 'categorical_feature_2']\n",
    "encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "data_encoded = encoder.fit_transform(data[categorical_features])\n",
    "\n",
    "# 10. Ingénierie de Caractéristiques\n",
    "# Supposons que vous créiez une nouvelle caractéristique basée sur des caractéristiques existantes\n",
    "data['nouvelle_caracteristique'] = data['caracteristique_1'] * data['caracteristique_2']\n",
    "\n",
    "# Séparation en caractéristiques (X) et cible (y)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Créer le pipeline avec le prétraitement et le modèle final\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraîner le pipeline sur les données d'entraînement\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Évaluer les performances du modèle sur les données de test\n",
    "accuracy = pipeline.score(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "```\n",
    "\n",
    "Dans ce code, j'ai ajouté des étapes pour gérer les valeurs manquantes, les valeurs aberrantes, supprimer les doublons, gérer les valeurs anormales et gérer les valeurs zéro. N'oubliez pas de remplacer `'votre_colonne'` par les noms réels des colonnes que vous traitez. Assurez-vous également d'ajuster les paramètres (comme le seuil pour la gestion des valeurs aberrantes) selon les besoins spécifiques de vos données."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47282a65",
   "metadata": {},
   "source": [
    "### Nouveau dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6623ecc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1832819701.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Prelude\\AppData\\Local\\Temp\\ipykernel_9692\\1832819701.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Voici le code qui vous permet d'obtenir le nouveau dataset prétraité :\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Voici le code qui vous permet d'obtenir le nouveau dataset prétraité :\n",
    "\n",
    "```python\n",
    "# ...\n",
    "# (Code de prétraitement précédent)\n",
    "\n",
    "# 11. Obtention du nouveau jeu de données prétraité\n",
    "X_preprocessed = pipeline.transform(X)\n",
    "\n",
    "# Créez un DataFrame pandas pour visualiser les données prétraitées\n",
    "X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=numeric_features + categorical_features_encoded)\n",
    "\n",
    "# Afficher les premières lignes du DataFrame pour vérifier les résultats\n",
    "print(X_preprocessed_df.head())\n",
    "```\n",
    "\n",
    "Ce code utilise le pipeline que nous avons construit précédemment et applique le prétraitement aux données `X`. Ensuite, il crée un nouveau DataFrame `X_preprocessed_df` à partir des données prétraitées.\n",
    "\n",
    "Assurez-vous de remplacer `numeric_features` et `categorical_features_encoded` par les noms réels de vos caractéristiques numériques et catégorielles encodées. En exécutant ce code, vous verrez les premières lignes du nouveau jeu de données après le prétraitement. Vous pouvez également utiliser d'autres méthodes de visualisation ou d'analyse pour examiner plus en détail les résultats du prétraitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25296bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "Pour automatiser le pipeline de prétraitement, vous pouvez encapsuler les étapes dans des fonctions ou des classes. Cela vous permettra de réutiliser et de personnaliser facilement le processus de prétraitement pour différents ensembles de données. Voici un exemple de comment vous pourriez procéder :\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def clean_data(data):\n",
    "    # Gestion des valeurs manquantes\n",
    "    imputer = SimpleImputer(strategy='mean')\n",
    "    data[['numerical_feature_1', 'numerical_feature_2']] = imputer.fit_transform(data[['numerical_feature_1', 'numerical_feature_2']])\n",
    "\n",
    "    # Gestion des Valeurs Aberrantes\n",
    "    # ... (Ajoutez ici les étapes de gestion des valeurs aberrantes)\n",
    "\n",
    "    # Suppression des Duplicatas\n",
    "    data = data.drop_duplicates()\n",
    "\n",
    "    # Gestion des Valeurs Anormales\n",
    "    # ... (Ajoutez ici les étapes de gestion des valeurs anormales)\n",
    "\n",
    "    # Gestion des Valeurs Zéro\n",
    "    # ... (Ajoutez ici les étapes de gestion des valeurs zéro)\n",
    "\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    # Normalisation et Standardisation\n",
    "    numeric_features = ['numerical_feature_1', 'numerical_feature_2']\n",
    "    scaler = StandardScaler()\n",
    "    data[numeric_features] = scaler.fit_transform(data[numeric_features])\n",
    "\n",
    "    # Encodage des Variables Catégorielles\n",
    "    categorical_features = ['categorical_feature_1', 'categorical_feature_2']\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "    data_encoded = encoder.fit_transform(data[categorical_features])\n",
    "\n",
    "    # Ingénierie de Caractéristiques\n",
    "    # Supposons que vous créiez une nouvelle caractéristique basée sur des caractéristiques existantes\n",
    "    data['nouvelle_caracteristique'] = data['caracteristique_1'] * data['caracteristique_2']\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_preprocessed_data(data):\n",
    "    # Créer le pipeline avec le prétraitement et le modèle final\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='mean')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                               ('classifier', RandomForestClassifier())])\n",
    "\n",
    "    # Séparation en caractéristiques (X) et cible (y)\n",
    "    X = data.drop('target', axis=1)\n",
    "    y = data['target']\n",
    "\n",
    "    # Entraîner le pipeline sur les données\n",
    "    pipeline.fit(X, y)\n",
    "\n",
    "    # Transformer les données avec le prétraitement\n",
    "    X_preprocessed = pipeline.transform(X)\n",
    "\n",
    "    # Créez un DataFrame pandas pour le jeu de données prétraité\n",
    "    X_preprocessed_df = pd.DataFrame(X_preprocessed, columns=numeric_features + categorical_features_encoded)\n",
    "\n",
    "    return X_preprocessed_df\n",
    "\n",
    "# Utilisation des fonctions\n",
    "data = load_data('votre_jeu_de_donnees.csv')\n",
    "cleaned_data = clean_data(data)\n",
    "preprocessed_data = preprocess_data(cleaned_data)\n",
    "```\n",
    "\n",
    "Dans cet exemple, j'ai créé des fonctions distinctes pour charger les données, nettoyer les données, prétraiter les données et obtenir le jeu de données prétraité. Chaque fonction accomplit une étape spécifique du processus de prétraitement. Vous pouvez maintenant utiliser ces fonctions avec différents ensembles de données en passant simplement le chemin du fichier CSV à la fonction `load_data`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
